{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6125d65-3eaa-4a30-b8f7-ed1aab5ff34f",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## SemEval 2025 Task 9: The Food Hazard Detection Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6d1b5-29b8-4b8f-86a4-92325266494d",
   "metadata": {},
   "source": [
    "The Food Hazard Detection task evaluates explainable classification systems for titles of food-incident reports collected from the web. The implementation below aims to address the aforementioned challenge, utilizing the RoBERTa model.\n",
    "\n",
    "<br>The Assignment refer to the following Sub-Tasks:\n",
    "<br>(ST1) Text classification for food hazard prediction, predicting the type of hazard and product.\n",
    "<br>(ST2) Food hazard and product “vector” detection, predicting the exact hazard and product.\n",
    "<br>Before we proceed, we need to present the necessary libraries and extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cff45-cfee-4278-8fb3-9d68da63fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch joblib nltk transformers datasets 'accelerate>=0.26.0'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b2416-0ce2-4065-a157-91901109d165",
   "metadata": {},
   "source": [
    "We also need to establish the preprocessing and data-cleaning steps that we followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bbc2e-3ba9-4216-b26f-9660ed975b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data = data[['title', 'text', 'hazard-category', 'product-category', 'hazard', 'product']]\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_text_extended(text):\n",
    "    # URL Removal\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Number Removal\n",
    "    text = re.sub(r'\\b\\d+-\\d+\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Special Character Removal\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Stop Words Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    # Whitespace Removal\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "train_data = pd.read_csv('incidents_train.csv')\n",
    "valid_data = pd.read_csv('incidents_valid.csv')\n",
    "test_data = pd.read_csv('incidents_test.csv')\n",
    "\n",
    "train_data = preprocess_data(train_data)\n",
    "valid_data = preprocess_data(valid_data)\n",
    "test_data = preprocess_data(test_data)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(clean_text_extended)\n",
    "valid_data['text'] = valid_data['text'].apply(clean_text_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2f378-1df5-46a9-b5d5-8d35d4a02164",
   "metadata": {},
   "source": [
    "# ST1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5818211-e979-43f6-b0fd-79fc79dd58a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update label encoders\n",
    "def update_label_encoders(encoder, data, column_name):\n",
    "\n",
    "    new_labels = set(data[column_name].unique()) - set(encoder.classes_)\n",
    "    if new_labels:\n",
    "        encoder.classes_ = np.append(encoder.classes_, list(new_labels))\n",
    "    labels = data[column_name].apply(lambda x: x if x in encoder.classes_ else 'unknown')\n",
    "    return encoder.transform(labels)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Conversion to Hugging Face Dataset\n",
    "def create_hf_dataset(df, labels):\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = labels\n",
    "    hf_dataset = HFDataset.from_pandas(df)\n",
    "    hf_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "    hf_dataset = hf_dataset.remove_columns([\"title\", \"text\", \"hazard-category\", \"product-category\"])\n",
    "    return hf_dataset\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"eval_f1\": f1_score(labels, preds, average=\"macro\")}\n",
    "\n",
    "\n",
    "def train_model(train_dataset, valid_dataset, num_labels):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(trainer, dataset, true_labels):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    return f1_score(true_labels, preds, average='macro')\n",
    "\n",
    "\n",
    "def get_predictions(trainer, dataset, label_encoder):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "    return decoded_preds\n",
    "\n",
    "\n",
    "label_encoder_hazard = LabelEncoder()\n",
    "label_encoder_product = LabelEncoder()\n",
    "\n",
    "hazard_labels_train = label_encoder_hazard.fit_transform(train_data['hazard-category'])\n",
    "product_labels_train = label_encoder_product.fit_transform(train_data['product-category'])\n",
    "\n",
    "hazard_labels_valid = update_label_encoders(label_encoder_hazard, valid_data, 'hazard-category')\n",
    "product_labels_valid = update_label_encoders(label_encoder_product, valid_data, 'product-category')\n",
    "\n",
    "hazard_labels_test = update_label_encoders(label_encoder_hazard, test_data, 'hazard-category')\n",
    "product_labels_test = update_label_encoders(label_encoder_product, test_data, 'product-category')\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset_hazard = create_hf_dataset(train_data, hazard_labels_train)\n",
    "valid_dataset_hazard = create_hf_dataset(valid_data, hazard_labels_valid)\n",
    "test_dataset_hazard = create_hf_dataset(test_data, hazard_labels_test)\n",
    "\n",
    "train_dataset_product = create_hf_dataset(train_data, product_labels_train)\n",
    "valid_dataset_product = create_hf_dataset(valid_data, product_labels_valid)\n",
    "test_dataset_product = create_hf_dataset(test_data, product_labels_test)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer_hazard = train_model(train_dataset_hazard, valid_dataset_hazard, num_labels=len(label_encoder_hazard.classes_))\n",
    "trainer_product = train_model(train_dataset_product, valid_dataset_product, num_labels=len(label_encoder_product.classes_))\n",
    "\n",
    "f1_hazard_valid = evaluate_model(trainer_hazard, valid_dataset_hazard, hazard_labels_valid)\n",
    "f1_product_valid = evaluate_model(trainer_product, valid_dataset_product, product_labels_valid)\n",
    "\n",
    "final_score_valid = (f1_hazard_valid + f1_product_valid) / 2.0\n",
    "\n",
    "print(f\"Validation F1 Score for Hazard: {f1_hazard_valid}\")\n",
    "print(f\"Validation F1 Score for Product: {f1_product_valid}\")\n",
    "print(f\"Validation Final Score: {final_score_valid}\")\n",
    "\n",
    "f1_hazard_test = evaluate_model(trainer_hazard, test_dataset_hazard, hazard_labels_test)\n",
    "f1_product_test = evaluate_model(trainer_product, test_dataset_product, product_labels_test)\n",
    "final_score_test = (f1_hazard_test + f1_product_test) / 2.0\n",
    "\n",
    "print(f\"Test F1 Score for Hazard: {f1_hazard_test}\")\n",
    "print(f\"Test F1 Score for Product: {f1_product_test}\")\n",
    "print(f\"Test Final Score: {final_score_test}\")\n",
    "\n",
    "test_hazard_preds = get_predictions(trainer_hazard, test_dataset_hazard, label_encoder_hazard)\n",
    "test_product_preds = get_predictions(trainer_product, test_dataset_product, label_encoder_product)\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    \"Index\": test_data.index,\n",
    "    \"Predicted_Hazard_Category\": test_hazard_preds,\n",
    "    \"Predicted_Product_Category\": test_product_preds\n",
    "})\n",
    "\n",
    "output_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9ba06-ffab-4831-8e63-5fe9ad1edc65",
   "metadata": {},
   "source": [
    "# ST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcf296-a72a-489d-ab4e-df9ebdd92ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update label encoders\n",
    "def update_label_encoders(encoder, data, column_name):\n",
    "\n",
    "    new_labels = set(data[column_name].unique()) - set(encoder.classes_)\n",
    "    if new_labels:\n",
    "        encoder.classes_ = np.append(encoder.classes_, list(new_labels))\n",
    "    labels = data[column_name].apply(lambda x: x if x in encoder.classes_ else 'unknown')\n",
    "    return encoder.transform(labels)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Conversion to Hugging Face Dataset\n",
    "def create_hf_dataset(df, labels):\n",
    "    df = df.copy()\n",
    "    df[\"labels\"] = labels  # Add labels explicitly to the dataset\n",
    "    hf_dataset = HFDataset.from_pandas(df)\n",
    "    hf_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
    "    hf_dataset = hf_dataset.remove_columns([\"title\", \"text\", \"hazard-category\", \"product-category\", \"hazard\", \"product\"])\n",
    "    return hf_dataset\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"eval_f1\": f1_score(labels, preds, average=\"macro\")}\n",
    "\n",
    "\n",
    "def train_model(train_dataset, valid_dataset, num_labels):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(trainer, dataset, true_labels):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    return f1_score(true_labels, preds, average='macro')\n",
    "\n",
    "\n",
    "# Decoding predictions function\n",
    "def get_predictions(trainer, dataset, label_encoder):\n",
    "    predictions = trainer.predict(dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    decoded_preds = label_encoder.inverse_transform(preds)\n",
    "    return decoded_preds\n",
    "\n",
    "\n",
    "label_encoder_hazard_vector = LabelEncoder()\n",
    "label_encoder_product_vector = LabelEncoder()\n",
    "\n",
    "hazard_vector_labels_train = label_encoder_hazard_vector.fit_transform(train_data['hazard'])\n",
    "product_vector_labels_train = label_encoder_product_vector.fit_transform(train_data['product'])\n",
    "\n",
    "hazard_vector_labels_valid = update_label_encoders(label_encoder_hazard_vector, valid_data, 'hazard')\n",
    "product_vector_labels_valid = update_label_encoders(label_encoder_product_vector, valid_data, 'product')\n",
    "\n",
    "hazard_vector_labels_test = update_label_encoders(label_encoder_hazard_vector, test_data, 'hazard')\n",
    "product_vector_labels_test = update_label_encoders(label_encoder_product_vector, test_data, 'product')\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset_hazard_vector = create_hf_dataset(train_data, hazard_vector_labels_train)\n",
    "valid_dataset_hazard_vector = create_hf_dataset(valid_data, hazard_vector_labels_valid)\n",
    "test_dataset_hazard_vector = create_hf_dataset(test_data, hazard_vector_labels_test)\n",
    "\n",
    "train_dataset_product_vector = create_hf_dataset(train_data, product_vector_labels_train)\n",
    "valid_dataset_product_vector = create_hf_dataset(valid_data, product_vector_labels_valid)\n",
    "test_dataset_product_vector = create_hf_dataset(test_data, product_vector_labels_test)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# Model Training for ST2\n",
    "trainer_hazard_vector = train_model(train_dataset_hazard_vector, valid_dataset_hazard_vector, num_labels=len(label_encoder_hazard_vector.classes_))\n",
    "trainer_product_vector = train_model(train_dataset_product_vector, valid_dataset_product_vector, num_labels=len(label_encoder_product_vector.classes_))\n",
    "\n",
    "f1_hazard_vector_valid = evaluate_model(trainer_hazard_vector, valid_dataset_hazard_vector, hazard_vector_labels_valid)\n",
    "f1_product_vector_valid = evaluate_model(trainer_product_vector, valid_dataset_product_vector, product_vector_labels_valid)\n",
    "final_score_valid_st2 = (f1_hazard_vector_valid + f1_product_vector_valid) / 2.0\n",
    "\n",
    "print(f\"Validation F1 Score for Hazard Vector (ST2): {f1_hazard_vector_valid}\")\n",
    "print(f\"Validation F1 Score for Product Vector (ST2): {f1_product_vector_valid}\")\n",
    "print(f\"Validation Final Score (ST2): {final_score_valid_st2}\")\n",
    "\n",
    "f1_hazard_vector_test = evaluate_model(trainer_hazard_vector, test_dataset_hazard_vector, hazard_vector_labels_test)\n",
    "f1_product_vector_test = evaluate_model(trainer_product_vector, test_dataset_product_vector, product_vector_labels_test)\n",
    "final_score_test_st2 = (f1_hazard_vector_test + f1_product_vector_test) / 2.0\n",
    "\n",
    "print(f\"Test F1 Score for Hazard Vector (ST2): {f1_hazard_vector_test}\")\n",
    "print(f\"Test F1 Score for Product Vector (ST2): {f1_product_vector_test}\")\n",
    "print(f\"Test Final Score (ST2): {final_score_test_st2}\")\n",
    "\n",
    "# Predicted labels for ST2\n",
    "test_hazard_vector_preds = get_predictions(trainer_hazard_vector, test_dataset_hazard_vector, label_encoder_hazard_vector)\n",
    "test_product_vector_preds = get_predictions(trainer_product_vector, test_dataset_product_vector, label_encoder_product_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a515fc-da48-4029-96dc-754e62b96eb0",
   "metadata": {},
   "source": [
    "# File Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf3732-1ad5-41c9-b46a-d5922a48e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with ST1 predictions\n",
    "st1_df = pd.DataFrame({\n",
    "    \"Index\": range(len(test_hazard_preds)),\n",
    "    \"Predicted_Hazard_Category\": test_hazard_preds,\n",
    "    \"Predicted_Product_Category\": test_product_preds\n",
    "})\n",
    "\n",
    "# DataFrame with ST2 predictions\n",
    "st2_df = pd.DataFrame({\n",
    "    \"Index\": range(len(test_hazard_vector_preds)),\n",
    "    \"Predicted_Hazard_Vector\": test_hazard_vector_preds,\n",
    "    \"Predicted_Product_Vector\": test_product_vector_preds\n",
    "})\n",
    "\n",
    "final_df = pd.merge(st1_df, st2_df, on=\"Index\")\n",
    "final_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Updated predictions saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef9f19-a5c0-484c-b676-6bf0076fe0d4",
   "metadata": {},
   "source": [
    "# File Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa1893b-5d77-4c13-aa33-a6b59077a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = \"submission.csv\"\n",
    "zip_filename = \"submission.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_filename)\n",
    "\n",
    "print(f\"Zipped file saved as {zip_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
